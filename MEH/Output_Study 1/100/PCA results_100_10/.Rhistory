R.version$year
#RYAN BOYD, Lancaster University, last updated on 2020-03-24; Katie Hoemann, KU Leuven, updated 2021-03-24
#----All input should be in .CSV format and MUST be in wide format for this script
#----This script requires the following packages (working as of R version 3.5.1):
# install.packages("psych")
# install.packages("GPArotation")
# install.packages("MASS")
# install.packages(c("FactoMineR","factoextra"))
# install.packages('data.table')
library(psych)
library(GPArotation)
library(MASS)
library("FactoMineR")
library("factoextra")
library(data.table)
library(changepoint)
#SET PARAMETERS
dataSet <- "ARI" # data set corresponding to folder name
nGrams <- 100 # number n-grams extracted
fileDate <- "2021-06-23" # date MEH file was generated
minWordCount <- 30 # minimum word count to analyze
numComponents <- 10 # number of components/factors to extract in PCA
setwd(paste0("C:/Users/Katie/Documents/MEH/",dataSet,"/",nGrams))
dirName <- paste("PCA results",nGrams,numComponents,sep='_')
dir.create(dirName, showWarnings = FALSE)
#LOAD DATA
BeginningColumn <- 5 #This script will do a PCA starting at this column and will include all variables to the end of your dataset
DF <- read.csv(paste0(fileDate,"_MEH_DTM_Binary.csv"), fileEncoding = 'UTF-8-BOM')
DF[BeginningColumn:length(DF)] <- apply(DF[BeginningColumn:length(DF)], 2, as.character)
DF[BeginningColumn:length(DF)] <- apply(DF[BeginningColumn:length(DF)], 2, as.numeric)
colnames(DF)[1] = 'Filename'
DF <- subset(DF, WC >= minWordCount) # grab only entries meeting minimum word count
#EXPLORATORY PCA TO DETERMINE NUMBER OF COMPONENTS
explorePCA <- PCA(DF[BeginningColumn:length(DF)],graph=FALSE)
exploreEVs <- get_eigenvalue(explorePCA)
exploreEVs <- as.data.frame(exploreEVs)
EVgt1 <- nrow(subset(exploreEVs, exploreEVs$eigenvalue >= 1))
EVgt2 <- nrow(subset(exploreEVs, exploreEVs$eigenvalue >= 2))
fviz_eig(explorePCA, ncp=EVgt1)
#DEFINE FUNCTIONS
kmo = function( data ) {
X <- cor(as.matrix(data))
iX <- ginv(X)
S2 <- diag(diag((iX^-1)))
AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix
IS <- X+AIS-2*S2                         # image covariance matrix
Dai <- sqrt(diag(diag(AIS)))
IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix
AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix
a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)
AA <- sum(a)
b <- apply((X - diag(nrow(X)))^2, 2, sum)
BB <- sum(b)
MSA <- b/(b+a)                        # indiv. measures of sampling adequacy
AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the correlation matrix. That is the  negative of the partial correlations, partialling out all other variables.
kmo <- BB/(AA+BB)                     # overall KMO statistic
# Reporting the conclusion
if (kmo >= 0.00 && kmo < 0.50){test <- 'The KMO test yields a POOR degree of common variance.'}
else if (kmo >= 0.50 && kmo < 0.60){test <- 'The KMO test yields a SOMEWHAT POOR degree of common variance.'}
else if (kmo >= 0.60 && kmo < 0.70){test <- 'The KMO test yields a DECENT degree of common variance.'}
else if (kmo >= 0.70 && kmo < 0.80){test <- 'The KMO test yields a GOOD degree of common variance.' }
else if (kmo >= 0.80 && kmo < 0.90){test <- 'The KMO test yields a VERY GOOD degree of common variance.' }
else { test <- 'The KMO test yields a FANTASTIC degree of common variance.' }
ans <- list( overall = kmo,
report = test,
individual = MSA,
AIS = AIS,
AIR = AIR )
return(ans)
}
Bartlett.sphericity.test <- function(x) {
method <- "Bartlett's test of sphericity"
data.name <- deparse(substitute(x))
x <- subset(x, complete.cases(x))
n <- nrow(x)
p <- ncol(x)
chisq <- (1-n+(2*p+5)/6)*log(det(cor(x)))
df <- p*(p-1)/2
p.value <- pchisq(chisq, df, lower.tail=FALSE)
names(chisq) <- "X-squared"
names(df) <- "df"
return(structure(list(statistic=chisq, parameter=df, p.value=p.value, method=method, data.name=data.name), class="htest"))
}
remove_zero_var <- function(dat) {
out <- lapply(dat, function(x) length(unique(x)))
want <- which(out > 1)
return(dat[, c(want)])
}
PCAFunction <- function(InputData, NumberOfFactors) {
InputData = remove_zero_var(InputData) #remove columns with no variance
options(width=10000)
options(max.print=2000000000)
cat("\n\nKMO_TEST:\n\n") #run the KMO test and print results
KMO_Results <- kmo(InputData)
cat(paste("KMO_METRIC: ", KMO_Results$overall, "\n", KMO_Results$report, sep=""))
cat("\n\nBARTLETT_SPHERICITY_TEST:\n\n") #same for the Bartlett test of sphericity
print(Bartlett.sphericity.test(InputData))
cat("\n\nFACTOR_ANALYSIS_RESULTS:\n\n") #run the PCA and print results
PCA <- principal(InputData, nfactors=NumberOfFactors, residuals=FALSE, rotate="varimax", method="regression")
print(PCA)
return(PCA)
}
#RUN PCA
sink(paste(dirName, "/", Sys.Date(), "_-_PCA_Results.txt", sep="")) #This is where you call the PCA. This saves all results to a file.
PCA_Results = PCAFunction(DF[BeginningColumn:length(DF)], NumberOfFactors=numComponents)
sink()
PCA_Scores_DF = data.frame(PCA_Results$scores)
PCA_Scores_DF$Filename = as.character(DF$Filename)
write.csv(PCA_Results$weights, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Weights.csv", sep=""))
write.csv(PCA_Results$loadings, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Loadings.csv", sep=""))
write.csv(PCA_Scores_DF, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Scores.csv", sep=""), row.names=F)
#SCORE TEXTS FOR PCA COMPONENTS
DF = data.frame(fread(paste0(fileDate,"_MEH_DTM_Verbose.csv"))) #read in the VERBOSE (i.e., "relative frequency") document term matrix
setwd(paste0("C:/Users/Katie/Documents/MEH/",dataSet,"/",nGrams,"/","PCA results","_",nGrams,"_",numComponents)) #jump to PCA results directory
loadings = read.csv(paste0(fileDate,"_-_PCA_Results - Loadings.csv")) #read in the loadings file from the PCA
colnames(loadings)[1] = 'Term' #set the first column of the loading matrix to have the variable name "Term"
loadings[,c(2:ncol(loadings))][ abs(loadings[,c(2:ncol(loadings))]) < .10] = NA #suppress small values
DF_Scored = data.frame(DF$Filename) #create an empty data frame to fill out
colnames(DF_Scored)[1] = 'Filename'
DF_Scored$Filename = as.character(DF_Scored$Filename)
#this loop goes through and calculates everything that we need
for(i in c(2:ncol(loadings))){
retain_terms = as.character(loadings[ , c("Term")][!is.na(loadings[ , i])])
term_loadings = loadings[ , c(i)][!is.na(loadings[ , i])]
DF_Scored[ ,c(colnames(loadings)[i])] = 0 #go in and actually score the texts
for(j in c(1:length(retain_terms))){
if(term_loadings[j] > 0){
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] + DF[ , c(retain_terms[j])]
}
else
{
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] - DF[ , c(retain_terms[j])]
}
}
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] #put on a percentage scale analogous to LIWC
}
#take the original data set, keep only the filenames and Word Count and keep the theme scores
DF = DF[ , c('Filename', 'WC')]
DF_Scored = merge(DF, DF_Scored, by='Filename')
write.csv(DF_Scored, 'MEM Themes - Dataset - Scored via Relative Frequencies.csv', row.names=F, na='') #write out our dataset
#calculate descriptive statistics and save them as an output file
descriptives = describe(DF_Scored)
write.csv(descriptives, 'MEM Themes - Descriptives.csv')
#CHANGEPOINTS ANALYSIS
DF = data.frame(fread("MEM Themes - Dataset - Scored via Relative Frequencies.csv", encoding='UTF-8'))
for (i_comp in 1:numComponents) {
colName <- paste0("RC",i_comp)
colData <- DF[,colName]
cpoints = cpt.mean(sort(colData[!is.na(colData)]), method="AMOC", Q=1); cp_low = cpoints@param.est$mean[1]; cp_high = cpoints@param.est$mean[length(cpoints@param.est$mean)];
DF[,colName] = 0; DF[colData >= cp_high,colName] = 1;
}
write.csv(DF, paste0("MEM Themes - Dataset - Scored via Relative Frequencies - Changepoints High_",nGrams,"_",numComponents,".csv"), row.names = F, na='')
#RYAN BOYD, Lancaster University, last updated on 2020-03-24; Katie Hoemann, KU Leuven, updated 2021-03-24
#----All input should be in .CSV format and MUST be in wide format for this script
#----This script requires the following packages (working as of R version 3.5.1):
# install.packages("psych")
# install.packages("GPArotation")
# install.packages("MASS")
# install.packages(c("FactoMineR","factoextra"))
# install.packages('data.table')
library(psych)
library(GPArotation)
library(MASS)
library("FactoMineR")
library("factoextra")
library(data.table)
library(changepoint)
#SET PARAMETERS
dataSet <- "ARI" # data set corresponding to folder name
nGrams <- 100 # number n-grams extracted
fileDate <- "2021-06-23" # date MEH file was generated
minWordCount <- 30 # minimum word count to analyze
numComponents <- 10 # number of components/factors to extract in PCA
setwd(paste0("C:/Users/Katie/Documents/MEH/",dataSet,"/",nGrams))
dirName <- paste("PCA results",nGrams,numComponents,sep='_')
dir.create(dirName, showWarnings = FALSE)
#LOAD DATA
BeginningColumn <- 5 #This script will do a PCA starting at this column and will include all variables to the end of your dataset
DF <- read.csv(paste0(fileDate,"_MEH_DTM_Binary.csv"), fileEncoding = 'UTF-8-BOM')
DF[BeginningColumn:length(DF)] <- apply(DF[BeginningColumn:length(DF)], 2, as.character)
DF[BeginningColumn:length(DF)] <- apply(DF[BeginningColumn:length(DF)], 2, as.numeric)
colnames(DF)[1] = 'Filename'
DF <- subset(DF, WC >= minWordCount) # grab only entries meeting minimum word count
#EXPLORATORY PCA TO DETERMINE NUMBER OF COMPONENTS
explorePCA <- PCA(DF[BeginningColumn:length(DF)],graph=FALSE)
exploreEVs <- get_eigenvalue(explorePCA)
exploreEVs <- as.data.frame(exploreEVs)
EVgt1 <- nrow(subset(exploreEVs, exploreEVs$eigenvalue >= 1))
EVgt2 <- nrow(subset(exploreEVs, exploreEVs$eigenvalue >= 2))
fviz_eig(explorePCA, ncp=EVgt1)
#DEFINE FUNCTIONS
kmo = function( data ) {
X <- cor(as.matrix(data))
iX <- ginv(X)
S2 <- diag(diag((iX^-1)))
AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix
IS <- X+AIS-2*S2                         # image covariance matrix
Dai <- sqrt(diag(diag(AIS)))
IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix
AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix
a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)
AA <- sum(a)
b <- apply((X - diag(nrow(X)))^2, 2, sum)
BB <- sum(b)
MSA <- b/(b+a)                        # indiv. measures of sampling adequacy
AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the correlation matrix. That is the  negative of the partial correlations, partialling out all other variables.
kmo <- BB/(AA+BB)                     # overall KMO statistic
# Reporting the conclusion
if (kmo >= 0.00 && kmo < 0.50){test <- 'The KMO test yields a POOR degree of common variance.'}
else if (kmo >= 0.50 && kmo < 0.60){test <- 'The KMO test yields a SOMEWHAT POOR degree of common variance.'}
else if (kmo >= 0.60 && kmo < 0.70){test <- 'The KMO test yields a DECENT degree of common variance.'}
else if (kmo >= 0.70 && kmo < 0.80){test <- 'The KMO test yields a GOOD degree of common variance.' }
else if (kmo >= 0.80 && kmo < 0.90){test <- 'The KMO test yields a VERY GOOD degree of common variance.' }
else { test <- 'The KMO test yields a FANTASTIC degree of common variance.' }
ans <- list( overall = kmo,
report = test,
individual = MSA,
AIS = AIS,
AIR = AIR )
return(ans)
}
Bartlett.sphericity.test <- function(x) {
method <- "Bartlett's test of sphericity"
data.name <- deparse(substitute(x))
x <- subset(x, complete.cases(x))
n <- nrow(x)
p <- ncol(x)
chisq <- (1-n+(2*p+5)/6)*log(det(cor(x)))
df <- p*(p-1)/2
p.value <- pchisq(chisq, df, lower.tail=FALSE)
names(chisq) <- "X-squared"
names(df) <- "df"
return(structure(list(statistic=chisq, parameter=df, p.value=p.value, method=method, data.name=data.name), class="htest"))
}
remove_zero_var <- function(dat) {
out <- lapply(dat, function(x) length(unique(x)))
want <- which(out > 1)
return(dat[, c(want)])
}
PCAFunction <- function(InputData, NumberOfFactors) {
InputData = remove_zero_var(InputData) #remove columns with no variance
options(width=10000)
options(max.print=2000000000)
cat("\n\nKMO_TEST:\n\n") #run the KMO test and print results
KMO_Results <- kmo(InputData)
cat(paste("KMO_METRIC: ", KMO_Results$overall, "\n", KMO_Results$report, sep=""))
cat("\n\nBARTLETT_SPHERICITY_TEST:\n\n") #same for the Bartlett test of sphericity
print(Bartlett.sphericity.test(InputData))
cat("\n\nFACTOR_ANALYSIS_RESULTS:\n\n") #run the PCA and print results
PCA <- principal(InputData, nfactors=NumberOfFactors, residuals=FALSE, rotate="varimax", method="regression")
print(PCA)
return(PCA)
}
#RUN PCA
sink(paste(dirName, "/", Sys.Date(), "_-_PCA_Results.txt", sep="")) #This is where you call the PCA. This saves all results to a file.
PCA_Results = PCAFunction(DF[BeginningColumn:length(DF)], NumberOfFactors=numComponents)
sink()
PCA_Scores_DF = data.frame(PCA_Results$scores)
PCA_Scores_DF$Filename = as.character(DF$Filename)
write.csv(PCA_Results$weights, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Weights.csv", sep=""))
write.csv(PCA_Results$loadings, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Loadings.csv", sep=""))
write.csv(PCA_Scores_DF, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Scores.csv", sep=""), row.names=F)
#SCORE TEXTS FOR PCA COMPONENTS
DF = data.frame(fread(paste0(fileDate,"_MEH_DTM_Verbose.csv"))) #read in the VERBOSE (i.e., "relative frequency") document term matrix
setwd(paste0("C:/Users/Katie/Documents/MEH/",dataSet,"/",nGrams,"/","PCA results","_",nGrams,"_",numComponents)) #jump to PCA results directory
loadings = read.csv(paste0(fileDate,"_-_PCA_Results - Loadings.csv")) #read in the loadings file from the PCA
colnames(loadings)[1] = 'Term' #set the first column of the loading matrix to have the variable name "Term"
loadings[,c(2:ncol(loadings))][ abs(loadings[,c(2:ncol(loadings))]) < .10] = NA #suppress small values
DF_Scored = data.frame(DF$Filename) #create an empty data frame to fill out
colnames(DF_Scored)[1] = 'Filename'
DF_Scored$Filename = as.character(DF_Scored$Filename)
#this loop goes through and calculates everything that we need
for(i in c(2:ncol(loadings))){
retain_terms = as.character(loadings[ , c("Term")][!is.na(loadings[ , i])])
term_loadings = loadings[ , c(i)][!is.na(loadings[ , i])]
DF_Scored[ ,c(colnames(loadings)[i])] = 0 #go in and actually score the texts
for(j in c(1:length(retain_terms))){
if(term_loadings[j] > 0){
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] + DF[ , c(retain_terms[j])]
}
else
{
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] - DF[ , c(retain_terms[j])]
}
}
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] #put on a percentage scale analogous to LIWC
}
#take the original data set, keep only the filenames and Word Count and keep the theme scores
DF = DF[ , c('Filename', 'WC')]
DF_Scored = merge(DF, DF_Scored, by='Filename')
write.csv(DF_Scored, 'MEM Themes - Dataset - Scored via Relative Frequencies.csv', row.names=F, na='') #write out our dataset
#calculate descriptive statistics and save them as an output file
descriptives = describe(DF_Scored)
write.csv(descriptives, 'MEM Themes - Descriptives.csv')
#CHANGEPOINTS ANALYSIS
DF = data.frame(fread("MEM Themes - Dataset - Scored via Relative Frequencies.csv", encoding='UTF-8'))
for (i_comp in 1:numComponents) {
colName <- paste0("RC",i_comp)
colData <- DF[,colName]
cpoints = cpt.mean(sort(colData[!is.na(colData)]), method="AMOC", Q=1); cp_low = cpoints@param.est$mean[1]; cp_high = cpoints@param.est$mean[length(cpoints@param.est$mean)];
DF[,colName] = 0; DF[colData >= cp_high,colName] = 1;
}
write.csv(DF, paste0("MEM Themes - Dataset - Scored via Relative Frequencies - Changepoints High_",nGrams,"_",numComponents,".csv"), row.names = F, na='')
#RYAN BOYD, Lancaster University, last updated on 2020-03-24; Katie Hoemann, KU Leuven, updated 2021-03-24
#----All input should be in .CSV format and MUST be in wide format for this script
#----This script requires the following packages (working as of R version 3.5.1):
# install.packages("psych")
# install.packages("GPArotation")
# install.packages("MASS")
# install.packages(c("FactoMineR","factoextra"))
# install.packages('data.table')
library(psych)
library(GPArotation)
library(MASS)
library("FactoMineR")
library("factoextra")
library(data.table)
library(changepoint)
#SET PARAMETERS
dataSet <- "ARI" # data set corresponding to folder name
nGrams <- 100 # number n-grams extracted
fileDate <- "2021-04-28" # date MEH file was generated
minWordCount <- 30 # minimum word count to analyze
numComponents <- 10 # number of components/factors to extract in PCA
setwd(paste0("C:/Users/Katie/Documents/MEH/",dataSet,"/",nGrams))
dirName <- paste("PCA results",nGrams,numComponents,sep='_')
dir.create(dirName, showWarnings = FALSE)
#LOAD DATA
BeginningColumn <- 5 #This script will do a PCA starting at this column and will include all variables to the end of your dataset
DF <- read.csv(paste0(fileDate,"_MEH_DTM_Binary.csv"), fileEncoding = 'UTF-8-BOM')
DF[BeginningColumn:length(DF)] <- apply(DF[BeginningColumn:length(DF)], 2, as.character)
DF[BeginningColumn:length(DF)] <- apply(DF[BeginningColumn:length(DF)], 2, as.numeric)
colnames(DF)[1] = 'Filename'
DF <- subset(DF, WC >= minWordCount) # grab only entries meeting minimum word count
#EXPLORATORY PCA TO DETERMINE NUMBER OF COMPONENTS
explorePCA <- PCA(DF[BeginningColumn:length(DF)],graph=FALSE)
exploreEVs <- get_eigenvalue(explorePCA)
exploreEVs <- as.data.frame(exploreEVs)
EVgt1 <- nrow(subset(exploreEVs, exploreEVs$eigenvalue >= 1))
EVgt2 <- nrow(subset(exploreEVs, exploreEVs$eigenvalue >= 2))
fviz_eig(explorePCA, ncp=EVgt1)
#DEFINE FUNCTIONS
kmo = function( data ) {
X <- cor(as.matrix(data))
iX <- ginv(X)
S2 <- diag(diag((iX^-1)))
AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix
IS <- X+AIS-2*S2                         # image covariance matrix
Dai <- sqrt(diag(diag(AIS)))
IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix
AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix
a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)
AA <- sum(a)
b <- apply((X - diag(nrow(X)))^2, 2, sum)
BB <- sum(b)
MSA <- b/(b+a)                        # indiv. measures of sampling adequacy
AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the correlation matrix. That is the  negative of the partial correlations, partialling out all other variables.
kmo <- BB/(AA+BB)                     # overall KMO statistic
# Reporting the conclusion
if (kmo >= 0.00 && kmo < 0.50){test <- 'The KMO test yields a POOR degree of common variance.'}
else if (kmo >= 0.50 && kmo < 0.60){test <- 'The KMO test yields a SOMEWHAT POOR degree of common variance.'}
else if (kmo >= 0.60 && kmo < 0.70){test <- 'The KMO test yields a DECENT degree of common variance.'}
else if (kmo >= 0.70 && kmo < 0.80){test <- 'The KMO test yields a GOOD degree of common variance.' }
else if (kmo >= 0.80 && kmo < 0.90){test <- 'The KMO test yields a VERY GOOD degree of common variance.' }
else { test <- 'The KMO test yields a FANTASTIC degree of common variance.' }
ans <- list( overall = kmo,
report = test,
individual = MSA,
AIS = AIS,
AIR = AIR )
return(ans)
}
Bartlett.sphericity.test <- function(x) {
method <- "Bartlett's test of sphericity"
data.name <- deparse(substitute(x))
x <- subset(x, complete.cases(x))
n <- nrow(x)
p <- ncol(x)
chisq <- (1-n+(2*p+5)/6)*log(det(cor(x)))
df <- p*(p-1)/2
p.value <- pchisq(chisq, df, lower.tail=FALSE)
names(chisq) <- "X-squared"
names(df) <- "df"
return(structure(list(statistic=chisq, parameter=df, p.value=p.value, method=method, data.name=data.name), class="htest"))
}
remove_zero_var <- function(dat) {
out <- lapply(dat, function(x) length(unique(x)))
want <- which(out > 1)
return(dat[, c(want)])
}
PCAFunction <- function(InputData, NumberOfFactors) {
InputData = remove_zero_var(InputData) #remove columns with no variance
options(width=10000)
options(max.print=2000000000)
cat("\n\nKMO_TEST:\n\n") #run the KMO test and print results
KMO_Results <- kmo(InputData)
cat(paste("KMO_METRIC: ", KMO_Results$overall, "\n", KMO_Results$report, sep=""))
cat("\n\nBARTLETT_SPHERICITY_TEST:\n\n") #same for the Bartlett test of sphericity
print(Bartlett.sphericity.test(InputData))
cat("\n\nFACTOR_ANALYSIS_RESULTS:\n\n") #run the PCA and print results
PCA <- principal(InputData, nfactors=NumberOfFactors, residuals=FALSE, rotate="varimax", method="regression")
print(PCA)
return(PCA)
}
#RUN PCA
sink(paste(dirName, "/", Sys.Date(), "_-_PCA_Results.txt", sep="")) #This is where you call the PCA. This saves all results to a file.
PCA_Results = PCAFunction(DF[BeginningColumn:length(DF)], NumberOfFactors=numComponents)
sink()
PCA_Scores_DF = data.frame(PCA_Results$scores)
PCA_Scores_DF$Filename = as.character(DF$Filename)
write.csv(PCA_Results$weights, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Weights.csv", sep=""))
write.csv(PCA_Results$loadings, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Loadings.csv", sep=""))
write.csv(PCA_Scores_DF, paste(dirName, "/", Sys.Date(), "_-_PCA_Results - Scores.csv", sep=""), row.names=F)
#SCORE TEXTS FOR PCA COMPONENTS
DF = data.frame(fread(paste0(fileDate,"_MEH_DTM_Verbose.csv"))) #read in the VERBOSE (i.e., "relative frequency") document term matrix
setwd(paste0("C:/Users/Katie/Documents/MEH/",dataSet,"/",nGrams,"/","PCA results","_",nGrams,"_",numComponents)) #jump to PCA results directory
loadings = read.csv(paste0(fileDate,"_-_PCA_Results - Loadings.csv")) #read in the loadings file from the PCA
colnames(loadings)[1] = 'Term' #set the first column of the loading matrix to have the variable name "Term"
loadings[,c(2:ncol(loadings))][ abs(loadings[,c(2:ncol(loadings))]) < .10] = NA #suppress small values
DF_Scored = data.frame(DF$Filename) #create an empty data frame to fill out
colnames(DF_Scored)[1] = 'Filename'
DF_Scored$Filename = as.character(DF_Scored$Filename)
#this loop goes through and calculates everything that we need
for(i in c(2:ncol(loadings))){
retain_terms = as.character(loadings[ , c("Term")][!is.na(loadings[ , i])])
term_loadings = loadings[ , c(i)][!is.na(loadings[ , i])]
DF_Scored[ ,c(colnames(loadings)[i])] = 0 #go in and actually score the texts
for(j in c(1:length(retain_terms))){
if(term_loadings[j] > 0){
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] + DF[ , c(retain_terms[j])]
}
else
{
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] - DF[ , c(retain_terms[j])]
}
}
DF_Scored[ ,c(colnames(loadings)[i])] = DF_Scored[ ,c(colnames(loadings)[i])] #put on a percentage scale analogous to LIWC
}
#take the original data set, keep only the filenames and Word Count and keep the theme scores
DF = DF[ , c('Filename', 'WC')]
DF_Scored = merge(DF, DF_Scored, by='Filename')
write.csv(DF_Scored, 'MEM Themes - Dataset - Scored via Relative Frequencies.csv', row.names=F, na='') #write out our dataset
#calculate descriptive statistics and save them as an output file
descriptives = describe(DF_Scored)
write.csv(descriptives, 'MEM Themes - Descriptives.csv')
#CHANGEPOINTS ANALYSIS
DF = data.frame(fread("MEM Themes - Dataset - Scored via Relative Frequencies.csv", encoding='UTF-8'))
for (i_comp in 1:numComponents) {
colName <- paste0("RC",i_comp)
colData <- DF[,colName]
cpoints = cpt.mean(sort(colData[!is.na(colData)]), method="AMOC", Q=1); cp_low = cpoints@param.est$mean[1]; cp_high = cpoints@param.est$mean[length(cpoints@param.est$mean)];
DF[,colName] = 0; DF[colData >= cp_high,colName] = 1;
}
write.csv(DF, paste0("MEM Themes - Dataset - Scored via Relative Frequencies - Changepoints High_",nGrams,"_",numComponents,".csv"), row.names = F, na='')
